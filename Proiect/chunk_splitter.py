import re
from langchain_text_splitters import RecursiveCharacterTextSplitter
from loader import load_data
import matplotlib.pyplot as plt
from tqdm import tqdm
from transformers import AutoTokenizer
import pandas as pd
from langchain_core.documents import Document as LangchainDocument


EMBEDDING_MODEL_NAME = "thenlper/gte-small"

def chunk_function(
    chunk_size: int,
    knowledge_base: list[LangchainDocument],
) -> list[LangchainDocument]:
    
   
    MARKDOWN_SEPARATORS = [
        r"\n{2,}",              # paragrafe separate
        r"\bDefinitia\b",       # FĂRĂ diacritice
        r"\bTeorema\b",         # Acesta era deja ok
        r"[0-9]+\.[0-9]+",      # subsectiuni numerotate
        r"\n",                  # linii noi
        r" "                    # fallback
    ]

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,  
        chunk_overlap=int(chunk_size / 10),
        add_start_index=True,
        strip_whitespace=True,
        separators=MARKDOWN_SEPARATORS,
    )

    docs_processed = []
    
    for doc in knowledge_base:
        docs_processed += text_splitter.split_documents([doc])


    unique_texts = {}
    docs_processed_unique = []
    for doc in docs_processed:
        if doc.page_content not in unique_texts:
            unique_texts[doc.page_content] = True
            docs_processed_unique.append(doc)

    return docs_processed_unique


docs_processed = chunk_function(256, load_data()) 

tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]
fig = pd.Series(lengths).hist()
plt.title("Distribution of document lengths in the knowledge base (in count of tokens)")
plt.xlabel("Tokens per chunk")
plt.ylabel("Number of chunks")

plt.savefig("chunk_distribution.png") 
print("Graficul distribuției chunk-urilor a fost salvat ca chunk_distribution.png")

# plt.show() 