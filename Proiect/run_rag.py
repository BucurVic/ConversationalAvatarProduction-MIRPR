import os
import time
import pickle
import faiss
import numpy as np
import subprocess
from pathlib import Path
from unidecode import unidecode
from llama_cpp import Llama
from sentence_transformers import SentenceTransformer 
import re

# --- Importuri Avatar (PÄƒstrate) ---
try:
    from scripts.config import (
        SADTALKER_REPO,
        USER_IMAGE,
        WAV2LIP_REPO
    )
    from generate_avatar.generate_tts import tts_piper
    from generate_avatar.generate_lip_sync_wav2lip import wav2lip_generate_video
except ImportError:
    print("[WARN] Nu s-au putut importa modulele de Avatar. RulÄƒm pe mock.")
    USER_IMAGE = "input_img.png"
    WAV2LIP_REPO = "external/Wav2Lip"
    def tts_piper(text, path): print(f"[MOCK TTS] Se genereazÄƒ audio la {path}...")
    def wav2lip_generate_video(**kwargs): print("[MOCK VIDEO] Se genereazÄƒ video..."); return "mock.mp4"

# --- CONFIGURARE ---
# Calea trebuie sÄƒ fie exact cea din create_vector_store.py
DB_FOLDER_PATH = 'vectorstoretmp/' 
EMBEDDING_MODEL_NAME = "thenlper/gte-small"
LLM_MODEL_PATH = "./models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"

# O clasÄƒ simplÄƒ care sÄƒ mimeze structura Document din LangChain
# (ca sÄƒ nu modificÄƒm funcÈ›ia create_prompt)
class SimpleDoc:
    def __init__(self, content, source="Manual"):
        self.page_content = content
        self.metadata = {'source': source}

# ----------------------------------------------------------
#            1. RESURSE (ÃŽncÄƒrcare ManualÄƒ)
# ----------------------------------------------------------
def load_resources():
    print(f"[INFO] Se Ã®ncarcÄƒ resursele...")
    
    # 1. ÃŽncÄƒrcÄƒm Indexul FAISS
    try:
        index = faiss.read_index(f"{DB_FOLDER_PATH}/index.faiss")
        print("[SUCCESS] Index FAISS Ã®ncÄƒrcat.")
    except Exception as e:
        print(f"[EROARE] Nu am putut citi index.faiss din {DB_FOLDER_PATH}: {e}")
        return None, None, None

    # 2. ÃŽncÄƒrcÄƒm Textele (Pickle)
    try:
        with open(f"{DB_FOLDER_PATH}/index.pkl", "rb") as f:
            texts = pickle.load(f)
        print(f"[SUCCESS] {len(texts)} fragmente de text Ã®ncÄƒrcate.")
    except Exception as e:
        print(f"[EROARE] Nu am putut citi index.pkl: {e}")
        return None, None, None

    # 3. ÃŽncÄƒrcÄƒm Modelul de Embedding (SentenceTransformer)
    # ÃŽl punem pe CPU ca sÄƒ nu ocupe VRAM-ul necesar pentru Llama
    print(f"[INFO] Se Ã®ncarcÄƒ modelul de embedding {EMBEDDING_MODEL_NAME}...")
    embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device="cuda")

    # 4. ÃŽncÄƒrcÄƒm Llama 3 (Pe GPU)
    print(f"[INFO] Se Ã®ncarcÄƒ Llama 3 8B...")
    try:
        llm = Llama(
            model_path=LLM_MODEL_PATH,
            n_gpu_layers=-1, # Totul pe GPU
            n_ctx=4096, 
            verbose=False
        )
        print("[SUCCESS] Llama 3 Ã®ncÄƒrcat pe GPU.")
    except Exception as e:
        print(f"[EROARE] Nu s-a putut Ã®ncÄƒrca Llama: {e}")
        return None, None, None

    return index, texts, embed_model, llm

# ----------------------------------------------------------
#            2. LOGICA DE RETRIEVAL MANUALÄ‚
# ----------------------------------------------------------
def search_manual(query, index, texts, embed_model, k=4):
    # 1. VectorizÄƒm Ã®ntrebarea
    query_vec = embed_model.encode([query]).astype("float32")
    
    # 2. CÄƒutÄƒm Ã®n FAISS
    distances, indices = index.search(query_vec, k)
    
    # 3. Extragem textele corespunzÄƒtoare indecÈ™ilor gÄƒsiÈ›i
    found_docs = []
    for idx in indices[0]:
        if 0 <= idx < len(texts):
            # CreÄƒm un obiect simplu compatibil cu restul codului
            # (create_vector_store-ul tÄƒu pÄƒstra doar textul, nu È™i sursa explicitÄƒ per chunk,
            # deci punem o sursÄƒ genericÄƒ sau Ã®ncercÄƒm sÄƒ o deducem dacÄƒ e Ã®n text)
            content = texts[idx]
            # ÃŽncercare simplÄƒ de a extrage titlul dacÄƒ e la Ã®nceput (Title: ...)
            source = "Manual"
            if content.startswith("Title:"):
                parts = content.split(":", 2)
                if len(parts) > 1:
                    source = parts[1].strip()
            
            found_docs.append(SimpleDoc(content, source))
            
    return found_docs

# ----------------------------------------------------------
#            3. RAG & PROMPT (Neschimbat)
# ----------------------------------------------------------
def create_prompt(context_docs, query):
    context = "\n\n".join([doc.page_content for doc in context_docs])
    
    prompt_template = f"""
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
EÈ™ti un Asistent Universitar AI expert, prietenos È™i rÄƒbdÄƒtor.

INSTRUCÈšIUNI STRICTE:
1. RÄƒspunsul tÄƒu trebuie sÄƒ fie bazat **EXCLUSIV** pe textul de la secÈ›iunea "CONTEXT" de mai jos. 
2. StructureazÄƒ rÄƒspunsul Ã®n douÄƒ pÄƒrÈ›i clare:
   a) **DefiniÈ›ia/RÄƒspunsul direct:** Preia informaÈ›ia exactÄƒ È™i riguroasÄƒ din text.
   b) **ExplicaÈ›ia simplÄƒ:** ReformuleazÄƒ pe scurt, "ca pentru studenÈ›i", ca sÄƒ fie uÈ™or de Ã®nÈ›eles.
3. DacÄƒ informaÈ›ia nu existÄƒ Ã®n context, spune sincer: "Nu am gÄƒsit aceastÄƒ informaÈ›ie Ã®n materialele de curs."
4. RÄƒspunde Ã®n limba romÃ¢nÄƒ.
<|eot_id|><|start_header_id|>user<|end_header_id|>
CONTEXT:
{context}

ÃŽNTREBAREA STUDENTULUI:
{query}

RÄ‚SPUNSUL TÄ‚U (Structurat):
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
    return prompt_template

def get_llm_response(prompt, llm_instance):
    if llm_instance is None: return None
    try:
        output = llm_instance.create_chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
        )
        return output['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"[EROARE] Eroare la generarea textului: {e}")
        return None

# ----------------------------------------------------------
#            4. AVATAR (Video)
# ----------------------------------------------------------
def generate_avatar_video_wav2lip(answer_text: str):
    text_for_tts = answer_text.replace("*", "").replace("#", "").replace("a)", "").replace("b)", "")
    base_dir = Path(__file__).resolve().parent
    work_dir = base_dir / "runtime" / "avatar_wav2lip"
    work_dir.mkdir(parents=True, exist_ok=True)
    audio_path = work_dir / "answer.wav"
    video_output_dir = work_dir / "video"
    video_output_dir.mkdir(parents=True, exist_ok=True)

    print("\n[INFO] Generare TTS (Piper)...")
    try:
        tts_piper(text_for_tts, str(audio_path))
    except Exception as e:
        print(f"[EROARE TTS] {e}")
        return None

    print("\n[INFO] Generare avatar cu Wav2Lip...")
    try:
        wav2lip_generate_video(
            image_path=str(base_dir / USER_IMAGE),
            audio_path=str(audio_path),
            output_dir=str(video_output_dir),
            wav2lip_repo=str(base_dir / WAV2LIP_REPO) 
        )
    except Exception as e:
        print(f"[EROARE Wav2Lip] {e}")
        return None

    mp4_files = list(video_output_dir.glob("*.mp4"))
    if mp4_files:
        final_video = max(mp4_files, key=lambda p: p.stat().st_mtime)
        print(f"\n[SUCCESS] Video Wav2Lip generat: {final_video}")
        try:
            subprocess.run(["xdg-open", str(final_video)])
        except:
            pass
        return final_video
    return None

# ----------------------------------------------------------
#                       MAIN
# ----------------------------------------------------------
if __name__ == "__main__":
    # 1. ÃŽncÄƒrcÄƒm resursele manual
    index, texts, embed_model, llm = load_resources()
    
    if not llm or not index:
        print("[EXIT] Resursele nu au putut fi Ã®ncÄƒrcate.")
        exit()
        
    print("\n" + "="*60)
    print("ðŸŽ“ TUTORE AI (Manual FAISS + Llama 3)")
    print(" Scrie 'exit' pentru a ieÈ™i.")
    print("="*60 + "\n")

    while True:
        query_original = input("\nÃŽntrebarea ta: ")
        
        if query_original.lower() in ['exit', 'quit']:
            break
            
        query_normalized = unidecode(query_original)
        
        # 2. CÄƒutare ManualÄƒ (FÄƒrÄƒ LangChain)
        context_docs = search_manual(query_normalized, index, texts, embed_model, k=4)
        
        # Extragere surse (simplificat)
        surse = set([d.metadata['source'] for d in context_docs])

        # 3. Generare RÄƒspuns
        prompt = create_prompt(context_docs, query_original)
        response_text = get_llm_response(prompt, llm)
        
        if response_text:
            print("\n" + "="*60)
            print("ðŸŽ“ RÄ‚SPUNS GENERAT:")
            print("-" * 60)
            print(response_text.strip())
            print("-" * 60)
            print("ðŸ“š SURSE POSIBILE:", list(surse)[:3])
            print("="*60 + "\n")
            
            # 4. Video (Extragem doar explicaÈ›ia simplÄƒ dacÄƒ se poate)
            text_pentru_avatar = response_text
            parts = re.split(r"\*\*ExplicaÈ›i[ea] simplÄƒ:\*\*", response_text, maxsplit=1)
            if len(parts) > 1:
                text_pentru_avatar = parts[1].strip()

            print(f"[INFO] Generare video...")
            generate_avatar_video_wav2lip(text_pentru_avatar)