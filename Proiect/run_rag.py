import time
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from unidecode import unidecode
from llama_cpp import Llama # <-- NOU: Biblioteca pentru rulare localÄƒ

# --- Configurare ---
DB_FAISS_PATH = 'vectorstore/'
EMBEDDING_MODEL_NAME = "thenlper/gte-small"

# Calea directÄƒ cÄƒtre modelul tÄƒu descÄƒrcat (schimbat de la LLM_MODEL la LLM_MODEL_PATH)
LLM_MODEL_PATH = "./models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"

def load_db():
    """ÃncarcÄƒ baza de date vectorialÄƒ FAISS."""
    print(f"[INFO] Se Ã®ncarcÄƒ baza de date...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cpu'})
    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
    return db

# --- NOU: FuncÈ›ie pentru a Ã®ncÄƒrca LLM-ul local ---
def load_llm():
    """ÃncarcÄƒ modelul LLM (GGUF) folosind llama-cpp-python."""
    print(f"[INFO] Se Ã®ncarcÄƒ modelul GPT-OSS-20B din: {LLM_MODEL_PATH}...")
    try:
        # n_gpu_layers=-1 foloseÈ™te GPU-ul la maxim (Metal/CUDA)
        llm = Llama(
            model_path=LLM_MODEL_PATH,
            n_gpu_layers=-1, 
            n_ctx=4096,
            verbose=False
        )
        print("[SUCCESS] Modelul LLM a fost Ã®ncÄƒrcat pe GPU.")
        return llm
    except Exception as e:
        print(f"[EROARE FATALÄ‚] Nu s-a putut Ã®ncÄƒrca modelul GGUF: {e}")
        print("AsigurÄƒ-te cÄƒ fiÈ™ierul existÄƒ la calea specificatÄƒ.")
        return None

def create_prompt(context_docs, query):
    """CreeazÄƒ promptul 'Tutore AI' structurat."""
    context = "\n\n".join([doc.page_content for doc in context_docs])
    
    prompt_template = f"""
EÈ™ti un Asistent Universitar AI expert, prietenos È™i rÄƒbdÄƒtor.

INSTRUCÈšIUNI STRICTE:
1. RÄƒspunsul tÄƒu trebuie sÄƒ fie bazat **EXCLUSIV** pe textul de la secÈ›iunea "CONTEXT" de mai jos. 
2. StructureazÄƒ rÄƒspunsul Ã®n douÄƒ pÄƒrÈ›i clare:
   a) **DefiniÈ›ia/RÄƒspunsul direct:** Preia informaÈ›ia exactÄƒ È™i riguroasÄƒ din text.
   b) **ExplicaÈ›ia simplÄƒ:** ReformuleazÄƒ pe scurt, "ca pentru studenÈ›i", ca sÄƒ fie uÈ™or de Ã®nÈ›eles.
3. DacÄƒ informaÈ›ia nu existÄƒ Ã®n context, spune sincer: "Nu am gÄƒsit aceastÄƒ informaÈ›ie Ã®n materialele de curs."
4. RÄƒspunde Ã®n limba romÃ¢nÄƒ.

CONTEXT DIN MANUAL:
---
{context}
---

ÃNTREBAREA STUDENTULUI:
{query}

RÄ‚SPUNSUL TÄ‚U (Structurat):
"""
    return prompt_template

# --- MODIFICAT: FuncÈ›ia de RÄƒspuns foloseÈ™te instanÈ›a localÄƒ LLM ---
def get_llm_response(prompt, llm_instance):
    """Trimite promptul cÄƒtre instanÈ›a Llama (local) È™i preia rÄƒspunsul curat."""
    if llm_instance is None: return None
    
    try:
        start_time = time.time()
        
        # Rulare localÄƒ
        output = llm_instance.create_chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        end_time = time.time()
        
        # Extragem textul brut
        raw_text = output['choices'][0]['message']['content']
        
        # --- FILTRARE PENTRU GPT-OSS-20B ---
        # Acest model pune rÄƒspunsul final dupÄƒ tag-ul: <|channel|>final<|message|>
        # VerificÄƒm dacÄƒ existÄƒ acest tag È™i tÄƒiem tot ce e Ã®nainte de el.
        marker = "<|channel|>final<|message|>"
        
        if marker in raw_text:
            # LuÄƒm ultima parte (dupÄƒ marker)
            clean_text = raw_text.split(marker)[-1]
        else:
            # DacÄƒ nu gÄƒseÈ™te markerul, Ã®nseamnÄƒ cÄƒ a rÄƒspuns direct (pÄƒstrÄƒm tot)
            clean_text = raw_text
            
        # CurÄƒÈ›Äƒm eventuale tag-uri de final rÄƒmase
        clean_text = clean_text.replace("<|end|>", "").strip()
        # -----------------------------------
        
        print(f"[INFO] Generare finalizatÄƒ Ã®n {end_time - start_time:.2f} secunde.")
        return clean_text

    except Exception as e:
        print(f"[EROARE] Eroare la generarea rÄƒspunsului: {e}")
        return None

# --- FuncÈ›ia principalÄƒ (ActualizatÄƒ) ---
if __name__ == "__main__":
    # 1. IniÈ›ializare
    db = load_db()
    llm = load_llm() # <-- ÃncÄƒrcÄƒm modelul local, nu clientul API
    
    if not llm:
        exit()
        
    print("\n" + "="*60)
    print("ğŸ“ TUTORE AI ACTIVAT (Local GPT-OSS/Llama)")
    print(f" Model: {LLM_MODEL_PATH.split('/')[-1]}")
    print(" Scrie 'exit' pentru a ieÈ™i.")
    print("="*60 + "\n")

    # 2. BuclÄƒ interactivÄƒ
    while True:
        query_original = input("\nÃntrebarea ta: ")
        
        if query_original.lower() in ['exit', 'quit']:
            break
            
        # 3. Retrieval
        query_normalized = unidecode(query_original)
        context_docs = db.similarity_search(query_normalized, k=4)
        
        # Extragerea Surselor
        surse_gasite = set()
        for doc in context_docs:
            raw_source = doc.metadata.get('source', 'Manual')
            sursa_curata = " ".join(raw_source.split())
            surse_gasite.add(sursa_curata)

        # 4. Generare RÄƒspuns
        prompt = create_prompt(context_docs, query_original)
        response = get_llm_response(prompt, llm) # <-- Trimitem la instanÈ›a LLM localÄƒ
        
        if response:
            print("\n" + "="*60)
            print("ğŸ“ RÄ‚SPUNS GENERAT:")
            print("-" * 60)
            print(response.strip())
            
            print("-" * 60)
            print("ğŸ“š SURSE BIBLIOGRAFICE:")
            sorted_sources = sorted(list(surse_gasite))
            
            for i, sursa in enumerate(sorted_sources):
                if i < 3:
                    print(f"   ğŸ“ {sursa}")
                else:
                    print(f"   ... (È™i altele)")
                    break
            print("="*60 + "\n")